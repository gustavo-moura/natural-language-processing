## BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

*A small summary of the paper in italics.*

**Authors:** Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
**Venue:** Name of the conference/journal where the paper was submitted (if any) 
**Year:** 24 May 2019

### Description

<!--This math is inline $`a^2+b^2=c^2`$.

This is on a separate line
```math
a^2+b^2=c^2
```
-->

improvement of fine-tuning based approaches

### Contribution
* Demonstrate the importance of bidirectional pre-training for language representations. BERT uses masked language models to enable pre-trained deep bidirectional representations
*  Show that pre-trained representations reduce the need for many heavily-engineered task-specific architectures.

*
